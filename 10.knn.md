_*算法图解笔记——Chapter 10 KNN*_  
_Author:    Seven Zou_  
_Email:     zoushiqi0404@gmail.com_  
_Language:  Python2.7_
* * *
### 5 K最近邻算法
K最近邻算法属于机器学习算法中的一种算法，也是我在课上选用来完成实践小组作业的算法。借助本章节，可以进行复习巩固。我个人类比KNN借用中国的一句谚语来总结就是“**近朱者赤近墨者黑**”，通过个体来确定周围**N**eighbor。针对数据，需要学习**特征抽取**；学习**回归**(预测数值)；学习K最近邻算法的应用案例和局限性。
* * *
#### 5.1 Case——推荐系统
假设需要为用户创建一个电影推荐系统。那么可以将所有用户放入一个图表中。
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405215440409.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E1MTg2,size_16,color_FFFFFF,t_70#pic_center)
 图中这些用户在图表中的位置取决于其喜好，所以喜好相似度高的用户，其距离越近。假设你要向Priyanka推荐电影，可以找出五位与他最接近的用户。
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405215856487.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E1MTg2,size_16,color_FFFFFF,t_70#pic_center)
 假设在对电影的喜好方面，Justin、JC、Joey、Lance和Chris都与Priyanka相似，那么他们喜欢的电影Priyanka可能也喜欢。那么问题就转化了，只要他们中任何一人喜欢的电影，就将其推荐给Priyanka。
##### 5.1.1 特征抽取
特征，按照我的理解就是不同数据的独有属性。比如水果，你可以选取它的个体和颜色特征进行比较。那么我们需要对数据进行特征抽取，然后再根据这些特征绘图。在图中，各个特征的数据变成了离散的点，而在度量两个特征(点)的距离。可以运用如下公式，其中$\left\| \cdot \right\|$表示为$2-norm$。
$$
 \left\|(x_1 - x_2)+(y_1 +y_2)\right\|
$$
既然运用图表，那么每位用户都会转换成为一组坐标放入其中。下面是一种转换方式。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405221631407.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2E1MTg2,size_16,color_FFFFFF,t_70#pic_center)
这里表达的是Priyanka和Justin都喜欢爱情片且都讨厌恐怖片。Morpheus喜欢动作片，但讨厌爱情片。那么在这里，每位用户所转换的坐标为Priyanka(3,4,4,1,4)、Justin(4,3,5,1,5)、Morpheus(2,5,1,3,1)。在这里做一个Remark，在数学中，这里计算的距离就变成了五维，但依然套用上述距离。
$$
\sqrt{(a_1 - a_2)^2+(b_1-b_2)^2+(c_1-c_2)^2+(d_1-d_2)^2+(e_1-e_2)^2}
$$
在这里的距离的意义在于指出两组数字之间的相似程度。那么根据计算两个用户的相似程度，比较其大小就可以更好地向你推荐电影。
* * *
##### 5.1.2 回归
假设不仅要给Priyanka推荐电影，还要预测她将给这部电影打多少分。那么就可以先找出她的5个邻居(这里数量自取)。与上subsection思想相似，我预测Priyanka会给电影打多少分，那么这里Justin、JC、Joey、Lance和Chris都打了多少分。
|姓名| 分数 |
|--|--|
| Justin |  5|
|JC | 4 |
| Joey | 4 |
| Lance | 5 |
| Chris | 3 |
|**Mean**|**4.2**|
求得平均值为4.2。这就是**回归**(regression)。KNN可以完成两项基本工作：1.分类(就是编组)；2.回归(就是预测结果)。

**Remark**：在计算两位用户的距离，使用的是距离公式。除此之外，在实际工作中，还会用到**余弦相似度**(cosine similarity)。余弦相似度不计算两个矢量的距离，而比较他们的角度。
##### 5.1.3 合适的特征
合适的特征选取对问题的解决十分重要。这里就不多叙述，不同的特征选取会直接影响求解结果，算法性能。
***
#### 5.2 机器学习个别实例
##### 5.2.1 OCR
OCR译指**光学字符识别**(optical character recognition)，例如针对拍摄印刷页面的照片，计算机将自动识别出其中的文字。这类场景在实际工作中应用地十分广泛。
假设图中有一数字。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200405223829366.png#pic_center)
那么利用KNN来自动识别的执行逻辑是：
-1.浏览大量的数字图像，将这些数字的特征提取出来；
-2.遇到新图像时，提取该图像的特征，再找出他的最近的邻居。

那么在应用中，一般OCR提取的特征时线段、点和曲线等特征，那么遇到新的字符时也提取同样的特征进行匹配。

OCR的第一步中查看大量的数字图像并提取特征，这就是**训练**(training)。大多数机器学习算法都包含训练。
##### 5.2.2 创建垃圾邮件过滤器
垃圾邮件过滤器使用的是——**朴素贝叶斯分类器**(Naive Bayes Classifier)，同样地，你也需要使用数据对分类器进行训练。
|主题| 是否垃圾邮件 |
|--|--|
| "Reset your password" | 不是 |
| "You have won 1 milllion dollars"|  是 |
| "Send me your password" | 是 |
| "Nigerian Prince sends you 10 million dollars" | 是 |
| "Happy Birthday" | 不是 |
那么假设收到了一封"collect your million dollar now！"邮件，根据这个句子中的单词，看看其在垃圾邮件中出现的概率。(朴素贝叶斯分类器可以计算出邮件为垃圾邮件的概率)
***
这便对KNN算法的思路进行了简单的回顾与介绍。对于KNN，可用于分类和回归问题。在特征抽取的过程中，合适的特征起到了决定性的作用。
***
### Reference
[美]Aditya Bhargava/袁国忠, 算法图解, 北京：人民邮电出版社, 2017.3.
* * *
附个人Github地址: [https://github.com/shiqi0404/Algorithm_Diagram](https://github.com/shiqi0404/Algorithm_Diagram)，其中包括笔记、Code还有书本pdf版。



